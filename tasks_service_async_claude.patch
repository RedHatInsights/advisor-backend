diff --git a/api/advisor/tasks/kafka_utils.py b/api/advisor/tasks/kafka_utils.py
index 228ab1a..65c7b82 100644
--- a/api/advisor/tasks/kafka_utils.py
+++ b/api/advisor/tasks/kafka_utils.py
@@ -14,6 +14,7 @@
 # You should have received a copy of the GNU General Public License along
 # with Insights Advisor. If not, see <https://www.gnu.org/licenses/>.
 
+import asyncio
 from confluent_kafka import Consumer, Producer
 from datetime import datetime
 import json
@@ -212,6 +213,8 @@ class KafkaDispatcher(object):
         self.loop_timeout = 1
         # Own consumer for own set of topics
         self.consumer = Consumer(kafka_settings.KAFKA_SETTINGS)
+        # Track running async tasks
+        self.running_tasks = set()
 
     def register_handler(self, topic, handler_fn, **kwargs):
         if topic in self.registered_handlers:
@@ -264,21 +267,32 @@ class KafkaDispatcher(object):
         except:
             logger.exception(f"Malformed JSON when handling {topic}")
             return
+
+        # Launch async handler as a task
+        handler = self.registered_handlers[topic]['handler']
+        task = asyncio.create_task(self._run_handler(handler, topic, body))
+        self.running_tasks.add(task)
+        task.add_done_callback(self.running_tasks.discard)
+
+    async def _run_handler(self, handler, topic, body):
+        """
+        Run a handler with Django request signals and error handling.
+        """
         # Tell Django we're starting a 'request' (db connection restarts...)
         request_started.send(sender=self.__class__)
-        # Call handler with JSON
         try:
-            self.registered_handlers[topic]['handler'](topic, body)
+            await handler(topic, body)
         except:
             logger.exception({
                 'message': "Error processing kafka message",
                 'topic': topic,
                 'payload': body
             })
-        # and we're finishing the 'request'
-        request_finished.send(sender=self.__class__)
+        finally:
+            # and we're finishing the 'request'
+            request_finished.send(sender=self.__class__)
 
-    def receive(self):
+    async def receive(self):
         """
         Run the receive loop continuously until told to stop.
         """
@@ -287,5 +301,14 @@ class KafkaDispatcher(object):
         while not self.quit:
             # longer polling timeouts mean fewer iterations through this loop,
             # but a longer time to respond to SIGTERM.  Here's the compromise:
-            self._handle_message(self.consumer.poll(self.loop_timeout))
+            message = self.consumer.poll(self.loop_timeout)
+            self._handle_message(message)
+            # Allow async tasks to run
+            await asyncio.sleep(0)
+
+        # Wait for all running tasks to complete before closing
+        if self.running_tasks:
+            logger.info(f"Waiting for {len(self.running_tasks)} tasks to complete")
+            await asyncio.gather(*self.running_tasks, return_exceptions=True)
+
         self.consumer.close()
diff --git a/api/advisor/tasks/management/commands/tasks_service.py b/api/advisor/tasks/management/commands/tasks_service.py
index beb474c..f807bf6 100644
--- a/api/advisor/tasks/management/commands/tasks_service.py
+++ b/api/advisor/tasks/management/commands/tasks_service.py
@@ -14,11 +14,13 @@
 # You should have received a copy of the GNU General Public License along
 # with Insights Advisor. If not, see <https://www.gnu.org/licenses/>.
 
+import asyncio
 import base64
 from json import loads, dumps
 import re
 import signal
 
+from asgiref.sync import sync_to_async
 from django.conf import settings
 from project_settings import kafka_settings
 from django.core.management.base import BaseCommand
@@ -58,7 +60,7 @@ JSON_DELIMITERS = {
 }
 
 
-def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
+async def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
     """
     Find the job for this run and try to process the stdout for it.  If
     something goes wrong,
@@ -66,8 +68,8 @@ def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
     delims = JSON_DELIMITERS[task_type]
 
     job.stdout = stdout
-    job.save()
-    job.new_log(True, f"got stdout from {delims['source']} for this job")
+    await job.asave()
+    await job.new_log(True, f"got stdout from {delims['source']} for this job")
     init_status = JobStatusChoices.FAILURE
     init_results = {
         'error': True, 'alert': True,
@@ -75,10 +77,10 @@ def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
     }
 
     # Allows us to exit early on failure but still set status and results.
-    def get_results(status, results):
+    async def get_results(status, results):
         if delims['prefix'] not in stdout:
             log_line = f"Could not find JSON prefix '{delims['prefix']}' in stdout"
-            job.new_log(False, log_line)
+            await job.new_log(False, log_line)
             results['message'] = log_line
             return status, results
         start = stdout.index(delims['prefix']) + len(delims['prefix'])
@@ -88,7 +90,7 @@ def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
             match = delims['suffixes'].search(stdout)
             if not match:
                 log_line = f"Could not find regular expression {delims['suffixes']} in stdout after prefix"
-                job.new_log(False, log_line)
+                await job.new_log(False, log_line)
                 results['message'] = log_line
                 return status, results
             # before } character, because we don't care about '"task_results": {'
@@ -99,7 +101,7 @@ def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
                     break
             else:
                 log_line = f"Could not find any of JSON suffix {delims['suffixes']} in stdout after prefix"
-                job.new_log(False, log_line)
+                await job.new_log(False, log_line)
                 results['message'] = log_line
                 return status, results
             end = stdout.index(suffix, start)
@@ -112,21 +114,21 @@ def parse_json_from_stdout(job, stdout, task_type: TaskTypeChoices):
                 # either what 'error' is set to, or False (no alert = fine)
                 results['alert'] = results.get('error', False)
             status = JobStatusChoices.SUCCESS
-            job.new_log(True, "Parsed JSON output successfully")
+            await job.new_log(True, "Parsed JSON output successfully")
         except Exception as exc:
             log_line = f"Error in parsing JSON between delimiters ({str(exc)}"
-            job.new_log(False, log_line)
+            await job.new_log(False, log_line)
             results['message'] = log_line
         return status, results
 
-    status, results = get_results(init_status, init_results)
+    status, results = await get_results(init_status, init_results)
     job.results = results
     job.status = status
-    job.save()
+    await job.asave()
 
 
 # Job update handler
-def handle_ansible_job_updates(topic, message):
+async def handle_ansible_job_updates(topic, message):
     """
     Message is of the form:
     {
@@ -173,7 +175,7 @@ def handle_ansible_job_updates(topic, message):
     updated_on = message['payload']['updated_at']
 
     try:
-        job = Job.objects.get(run_id=run_id)
+        job = await Job.objects.aget(run_id=run_id)
     except Job.DoesNotExist:
         logger.warn(
             'Job with run ID not found during Ansible playbook update processing when updating job status for task',
@@ -197,18 +199,18 @@ def handle_ansible_job_updates(topic, message):
         )
         new_status = JobStatusChoices[job.status]
 
-    update_job_status(job, new_status, updated_on)
+    await update_job_status(job, new_status, updated_on)
 
     if new_status == JobStatusChoices.RUNNING:
         # Don't process the stdout if we haven't finished running yet.
         return
-    stdout = fetch_playbook_dispatcher_stdout(job)
+    stdout = await fetch_playbook_dispatcher_stdout(job)
     if not stdout:
         return
-    parse_json_from_stdout(job, stdout, TaskTypeChoices.ANSIBLE)
+    await parse_json_from_stdout(job, stdout, TaskTypeChoices.ANSIBLE)
 
 
-def fetch_playbook_dispatcher_stdout(job):
+async def fetch_playbook_dispatcher_stdout(job):
     """
     This function retrieves the stdout of the playbook from a given run id.
     The auth header is constructed manually instead of auth_header_for_testing().
@@ -234,10 +236,10 @@ def fetch_playbook_dispatcher_stdout(job):
         }
     }
     ).encode())}
-    job.new_log(
+    await job.new_log(
         True, f'Requesting data from Playbook Dispatcher for run ID {job.run_id}'
     )
-    (response, elapsed) = retry_request(
+    (response, elapsed) = await sync_to_async(retry_request)(
         'Playbook Dispatcher',
         f'{settings.PLAYBOOK_DISPATCHER_URL}/api/playbook-dispatcher/v1/run_hosts'
         f'?fields[data]=stdout&filter[run][id]={job.run_id}',
@@ -249,7 +251,7 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error getting playbook-dispatcher stdout response',
             'status': response.status_code, 'text': response.text
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher returned {response.status_code} for run ID {job.run_id}'
         )
         return
@@ -259,7 +261,7 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error in parsing playbook-dispatcher stdout response',
             'original_text': response.text, 'problem': "Did not get a JSON object"
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher returned non-JSON response for run ID {job.run_id}'
         )
         return
@@ -268,7 +270,7 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error in parsing playbook-dispatcher stdout response',
             'original_text': response.text, 'problem': "'data' not found in JSON object"
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher returned JSON response with no data object for run ID {job.run_id}'
         )
         return
@@ -277,7 +279,7 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error in parsing playbook-dispatcher stdout response',
             'original_text': response.text, 'problem': "'data' value not a list"
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher JSON data is not a list for run ID {job.run_id}'
         )
         return
@@ -286,7 +288,7 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error in parsing playbook-dispatcher stdout response',
             'original_text': response.text, 'problem': "'data' list is empty"
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher an empty JSON data list for run ID {job.run_id}'
         )
         return
@@ -295,7 +297,7 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error in parsing playbook-dispatcher stdout response',
             'original_text': response.text, 'problem': "'data' list element 0 is not a JSON object"
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher JSON data list with non-object for run ID {job.run_id}'
         )
         return
@@ -304,14 +306,14 @@ def fetch_playbook_dispatcher_stdout(job):
             'message': 'Error in parsing playbook-dispatcher stdout response',
             'original_text': response.text, 'problem': "'data' list element 0 does not have the 'stdout' key"
         })
-        job.new_log(
+        await job.new_log(
             False, f'Playbook Dispatcher JSON data list item has no stdout data for run ID {job.run_id}'
         )
         return
     return json['data'][0]['stdout']
 
 
-def handle_script_job_updates(topic, message):
+async def handle_script_job_updates(topic, message):
     """
     Process the upload of the results of the rhc-worker-script via
     the payload tracker.
@@ -322,7 +324,7 @@ def handle_script_job_updates(topic, message):
         "ingress_kafka_message": message
     })
     updated_on = message['timestamp']
-    send_kafka_message(kafka_settings.PAYLOAD_TRACKER_TOPIC, {
+    await sync_to_async(send_kafka_message)(kafka_settings.PAYLOAD_TRACKER_TOPIC, {
         'status': 'received',
         'service': 'tasks',
         'source': 'rhc-worker-script',
@@ -335,7 +337,7 @@ def handle_script_job_updates(topic, message):
     })
     upload_url = message['url']
 
-    (request, duration) = retry_request('s3', upload_url)
+    (request, duration) = await sync_to_async(retry_request)('s3', upload_url)
 
     file_contents = request.json()  # We may need request.raw if it comes over as a gziped tar
 
@@ -347,7 +349,7 @@ def handle_script_job_updates(topic, message):
     stdout = file_contents['stdout']
 
     try:
-        job = Job.objects.get(run_id=run_id)
+        job = await Job.objects.aget(run_id=run_id)
     except Job.DoesNotExist:
         logger.warn(
             'Job with run ID not found during script update processing when updating job status for task',
@@ -359,11 +361,11 @@ def handle_script_job_updates(topic, message):
             }
         )
         return
-    parse_json_from_stdout(job, stdout, TaskTypeChoices.SCRIPT)
-    update_job_status(job, job.status, None)
+    await parse_json_from_stdout(job, stdout, TaskTypeChoices.SCRIPT)
+    await update_job_status(job, job.status, None)
 
 
-def update_job_status(job, new_status: JobStatusChoices, updated_on):
+async def update_job_status(job, new_status: JobStatusChoices, updated_on):
     """
     Does more than what it says on the tin:
     * Updates the job status and update time, if the status is different.
@@ -377,8 +379,8 @@ def update_job_status(job, new_status: JobStatusChoices, updated_on):
             job.updated_on = updated_datetime
         else:
             job.updated_on = timezone.now()
-        job.save()
-        job.new_log(
+        await job.asave()
+        await job.new_log(
             job.status == JobStatusChoices.SUCCESS,
             f"Updated job status to {job.get_status_display()}, update time {job.updated_on} (run ID {job.run_id})"
         )
@@ -395,7 +397,7 @@ def update_job_status(job, new_status: JobStatusChoices, updated_on):
     except Host.DoesNotExist:
         system_exists = False
     if system_exists and job.status in EVENT_TYPE_FOR_JOB_STATUS:
-        send_event_message(
+        await sync_to_async(send_event_message)(
             event_type=EVENT_TYPE_FOR_JOB_STATUS[job.status],
             org_id=job.executed_task.org_id,
             context={
@@ -409,17 +411,17 @@ def update_job_status(job, new_status: JobStatusChoices, updated_on):
                 'status': new_status.label,
             }]
         )
-        job.new_log(
+        await job.new_log(
             True,
             f"Sent message that this job is {job.get_status_display()} on system {display_name} (ID {job.system_id}, run ID {job.run_id})"
         )
     # Update the executed task status
-    update_executed_task_status(job.executed_task, job)
+    await update_executed_task_status(job.executed_task, job)
     logger.info("Tasks service update finished")
     return job
 
 
-def update_executed_task_status(
+async def update_executed_task_status(
     extask: ExecutedTask, job=None, send_message=True, delete_empty=False
 ):
     """
@@ -432,9 +434,11 @@ def update_executed_task_status(
     # We need the untrammeled Job list because otherwise it joins to Host
     # and that's unnecessary for these filters.
     job_statuses = set(
-        Job.objects.original_queryset().filter(
-            executed_task=extask
-        ).values_list('status', flat=True).distinct()
+        await sync_to_async(list)(
+            Job.objects.original_queryset().filter(
+                executed_task=extask
+            ).values_list('status', flat=True).distinct()
+        )
     )
     # Historically some executed tasks have ended up with no associated jobs.
     # We should deal with them early.
@@ -444,7 +448,7 @@ def update_executed_task_status(
             extask.name, extask.id, ("will" if delete_empty else "not set to")
         )
         if delete_empty:
-            extask.delete()
+            await extask.adelete()
         return
     # Executed tasks start in the RUNNING state, and jobs are timed out every
     # minute if RUNNING too long, so the only situation we need to deal with
@@ -457,12 +461,12 @@ def update_executed_task_status(
                 extask.name, extask.id
             )
             extask.status = ExecutedTaskStatusChoices.RUNNING
-            extask.save()
+            await extask.asave()
         return
     # If we're given a job, at this point it is the last job to change
     # state, so it can get a special message!
     if job is not None:
-        job.new_log(
+        await job.new_log(
             True,
             f"No more jobs running for executed task {job.executed_task.name} (ID {job.executed_task_id})"
         )
@@ -489,9 +493,9 @@ def update_executed_task_status(
     )
     extask.status = new_status
     extask.end_time = timezone.now()
-    extask.save()
+    await extask.asave()
     if send_message:
-        send_event_message(
+        await sync_to_async(send_event_message)(
             event_type='executed-task-completed',
             org_id=extask.org_id,
             context={},
@@ -504,29 +508,29 @@ def update_executed_task_status(
         )
 
 
-def get_satellite_source_type_id():
+async def get_satellite_source_type_id():
     """
     Gets the ID source_type for satellite from the sources api. Since this value never changes for each
      environment, it is stored in the django cache for future accesses.
     """
-    satellite_source_type_id = cache.get('satellite_source_type_id')
+    satellite_source_type_id = await sync_to_async(cache.get)('satellite_source_type_id')
     if satellite_source_type_id:
         return satellite_source_type_id
     auth_header = auth_header_for_testing(
         account=settings.SOURCE_API_ACCOUNT, org_id=settings.SOURCE_API_ORG,
         supply_http_header=True
     )
-    (response, elapsed) = retry_request(
+    (response, elapsed) = await sync_to_async(retry_request)(
         'sources api',
         f"{settings.SOURCES_API_URL}/api/sources/v3.1/source_types?filter[name]=satellite",
         headers=auth_header
     )
     satellite_source_type_id = int(response.json()['data'][0]['id'])
-    cache.set('satellite_source_type_id', satellite_source_type_id)
+    await sync_to_async(cache.set)('satellite_source_type_id', satellite_source_type_id)
     return satellite_source_type_id
 
 
-def handle_sources_event(topic, message):
+async def handle_sources_event(topic, message):
     """
     messages that we care about are of the form
     {
@@ -564,18 +568,18 @@ def handle_sources_event(topic, message):
     """
     if 'source_type_id' in message and 'source_ref' in message:
         # the satellite_source_message
-        satellite_source_type_id = get_satellite_source_type_id()
+        satellite_source_type_id = await get_satellite_source_type_id()
         if message['source_type_id'] == satellite_source_type_id and is_valid_uuid(message['source_ref']):
-            SatelliteRhc.objects.update_or_create(
+            await SatelliteRhc.objects.aupdate_or_create(
                 instance_id=message['source_ref'],
                 defaults={'source_id': message['id']}
             )
             return
     if 'rhc_id' in message and 'source_ids' in message:
         # the rhc_source_message
-        SatelliteRhc.objects.filter(
+        await SatelliteRhc.objects.filter(
             source_id=message['source_ids'][0],
-        ).update(
+        ).aupdate(
             rhc_client_id=message['rhc_id']
         )
 
@@ -604,6 +608,6 @@ class Command(BaseCommand):
         signal.signal(signal.SIGTERM, terminate)
         signal.signal(signal.SIGINT, terminate)
 
-        # Loops until receiver.quit is set
-        receiver.receive()
+        # Run the async receive loop
+        asyncio.run(receiver.receive())
         logger.info('Tasks service shutting down')
diff --git a/api/advisor/tasks/models.py b/api/advisor/tasks/models.py
index a48f3be..569c41c 100644
--- a/api/advisor/tasks/models.py
+++ b/api/advisor/tasks/models.py
@@ -231,13 +231,13 @@ class Job(models.Model):
     def __str__(self):
         return f"{self.executed_task.task.title} run on {self.system.display_name}: {self.get_status_display()}"
 
-    def new_log(self, is_ok, line):
+    async def new_log(self, is_ok, line):
         """
         Write a new log line for this job.  Used mainly for logging the
         internal processes for a job.
         """
         log = JobLog(job=self, is_ok=is_ok, line=line)
-        log.save()
+        await log.asave()
 
 
 class JobLog(models.Model):
diff --git a/api/advisor/tasks/tests/test_playbook_run_update.py b/api/advisor/tasks/tests/test_playbook_run_update.py
index e3e5227..22c3df3 100644
--- a/api/advisor/tasks/tests/test_playbook_run_update.py
+++ b/api/advisor/tasks/tests/test_playbook_run_update.py
@@ -18,6 +18,7 @@ import base64
 import json
 
 import responses
+from asgiref.sync import async_to_sync
 from project_settings import kafka_settings as kafka_settings
 from django.test import TestCase, override_settings
 from django.urls import reverse
@@ -164,7 +165,7 @@ class TaskJobUpdateTestCase(TestCase):
             status=200,
             json=json_playbook_dispatcher_reply()
         )
-        handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
+        async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
         request = responses.calls[0].request
         auth_header = json.loads(base64.b64decode(request.headers['x-rh-identity']))
         self.assertTrue(auth_header['identity']['user']['is_org_admin'])
@@ -225,7 +226,7 @@ class TaskJobUpdateTestCase(TestCase):
             status=200,
             json=json_playbook_dispatcher_reply_eoln()
         )
-        handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
+        async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
         res = self.client.get(
             reverse('tasks-executedtask-detail', kwargs={'id': constants.executed_task_id}),
             **self.std_auth
@@ -289,7 +290,7 @@ class TaskJobUpdateTestCase(TestCase):
         job.system_id = '00000000-0000-0000-0000-000000000000'  # non existent system.
         job.save()
 
-        handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
+        async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
 
         res = self.client.get(
             reverse('tasks-executedtask-detail', kwargs={'id': constants.executed_task_id}),
@@ -312,7 +313,7 @@ class TaskJobUpdateTestCase(TestCase):
         )
         update_message = run_update_message()
         update_message['payload']['id'] = constants.job_3_run_id
-        handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, update_message)
+        async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, update_message)
         request = responses.calls[0].request
         auth_header = json.loads(base64.b64decode(request.headers['x-rh-identity']))
         self.assertFalse(auth_header['identity']['user']['is_org_admin'])
@@ -322,7 +323,7 @@ class TaskJobUpdateTestCase(TestCase):
         update_message['payload']['status'] = "bad status"
         job_count = Job.objects.count()
         with self.assertRaises(KeyError):
-            handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, update_message)
+            async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, update_message)
         # Should be no extra jobs
         self.assertEqual(Job.objects.count(), job_count)
 
@@ -338,7 +339,7 @@ class TaskJobUpdateTestCase(TestCase):
             status=200,
             json=mangled_json_reply
         )
-        handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
+        async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
 
         res = self.client.get(
             reverse('tasks-executedtask-detail', kwargs={'id': constants.executed_task_id}),
@@ -369,7 +370,7 @@ class TaskJobUpdateTestCase(TestCase):
         # So the handle_ansible_job_updates function should call fetch_playbook_dispatcher_stdout because
         # the status is 'success'.  That then tries to fetch the stdout from
         # the playbook dispatcher, which gets a non-200 and fails.
-        handle_ansible_job_updates(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
+        async_to_sync(handle_ansible_job_updates)(kafka_settings.WEBHOOKS_TOPIC, run_update_message())
 
         # In that case we can still request the detail for the executed task...
         res = self.client.get(
@@ -401,7 +402,7 @@ class TaskJobUpdateTestCase(TestCase):
             json=mangled_json_reply
         )
         with self.assertLogs(logger='advisor-log', level='ERROR') as log:
-            self.assertIsNone(fetch_playbook_dispatcher_stdout(this_job))
+            self.assertIsNone(async_to_sync(fetch_playbook_dispatcher_stdout)(this_job))
             self.assertIn("'data' not found in JSON object", log.output[0])
 
         mangled_json_reply['data'] = "fruitcake"
@@ -412,7 +413,7 @@ class TaskJobUpdateTestCase(TestCase):
             json=mangled_json_reply
         )
         with self.assertLogs(logger='advisor-log', level='ERROR') as log:
-            self.assertIsNone(fetch_playbook_dispatcher_stdout(this_job))
+            self.assertIsNone(async_to_sync(fetch_playbook_dispatcher_stdout)(this_job))
             self.assertIn("'data' value not a list", log.output[0])
 
         mangled_json_reply['data'] = []
@@ -423,7 +424,7 @@ class TaskJobUpdateTestCase(TestCase):
             json=mangled_json_reply
         )
         with self.assertLogs(logger='advisor-log', level='ERROR') as log:
-            self.assertIsNone(fetch_playbook_dispatcher_stdout(this_job))
+            self.assertIsNone(async_to_sync(fetch_playbook_dispatcher_stdout)(this_job))
             self.assertIn("'data' list is empty", log.output[0])
 
         mangled_json_reply['data'] = ["failure"]
@@ -434,7 +435,7 @@ class TaskJobUpdateTestCase(TestCase):
             json=mangled_json_reply
         )
         with self.assertLogs(logger='advisor-log', level='ERROR') as log:
-            self.assertIsNone(fetch_playbook_dispatcher_stdout(this_job))
+            self.assertIsNone(async_to_sync(fetch_playbook_dispatcher_stdout)(this_job))
             self.assertIn("'data' list element 0 is not a JSON object", log.output[0])
 
         mangled_json_reply['data'] = [{"failure": True}]
@@ -445,7 +446,7 @@ class TaskJobUpdateTestCase(TestCase):
             json=mangled_json_reply
         )
         with self.assertLogs(logger='advisor-log', level='ERROR') as log:
-            self.assertIsNone(fetch_playbook_dispatcher_stdout(this_job))
+            self.assertIsNone(async_to_sync(fetch_playbook_dispatcher_stdout)(this_job))
             self.assertIn("'data' list element 0 does not have the 'stdout' key", log.output[0])
 
         mangled_json_reply = "Failure"
@@ -456,5 +457,5 @@ class TaskJobUpdateTestCase(TestCase):
             json=mangled_json_reply
         )
         with self.assertLogs(logger='advisor-log', level='ERROR') as log:
-            self.assertIsNone(fetch_playbook_dispatcher_stdout(this_job))
+            self.assertIsNone(async_to_sync(fetch_playbook_dispatcher_stdout)(this_job))
             self.assertIn("Did not get a JSON object", log.output[0])
diff --git a/api/advisor/tasks/tests/test_satellite_tracking.py b/api/advisor/tasks/tests/test_satellite_tracking.py
index 6924b59..00ccd93 100644
--- a/api/advisor/tasks/tests/test_satellite_tracking.py
+++ b/api/advisor/tasks/tests/test_satellite_tracking.py
@@ -17,6 +17,7 @@
 import responses
 from uuid import UUID
 
+from asgiref.sync import async_to_sync
 from project_settings import kafka_settings as kafka_settings
 from django.test import TestCase, override_settings
 
@@ -147,13 +148,13 @@ class TaskSatTrackingTestCase(TestCase):
 
     @responses.activate
     def test_correlate_satellite_to_rhc(self):
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, satellite_source_message())
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, satellite_source_message())
         sr = SatelliteRhc.objects.get(instance_id='357b7360-c0d6-11ec-a1f5-abea1b2200b3')
         self.assertEqual(sr.instance_id, UUID('357b7360-c0d6-11ec-a1f5-abea1b2200b3'))
         self.assertEqual(sr.source_id, 147)
         self.assertIsNone(sr.rhc_client_id)
 
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, rhc_source_message())
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, rhc_source_message())
         sr = SatelliteRhc.objects.get(instance_id='357b7360-c0d6-11ec-a1f5-abea1b2200b3')
         self.assertEqual(sr.instance_id, UUID('357b7360-c0d6-11ec-a1f5-abea1b2200b3'))
         self.assertEqual(sr.source_id, 147)
@@ -166,12 +167,12 @@ class TaskSatTrackingTestCase(TestCase):
         message = satellite_source_message()
         message['source_ref'] = 'bogus non uuid'
         # The test is that no exception is raised - so no error is generated.
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, message)
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, message)
 
     @responses.activate
     def test_update_satellite_source_id(self):
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, satellite_source_message())
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, rhc_source_message())
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, satellite_source_message())
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, rhc_source_message())
 
         sr = SatelliteRhc.objects.get(instance_id='357b7360-c0d6-11ec-a1f5-abea1b2200b3')
         self.assertEqual(sr.source_id, 147)
@@ -180,7 +181,7 @@ class TaskSatTrackingTestCase(TestCase):
         message_new_id = satellite_source_message()
         message_new_id['id'] = 1337
 
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, message_new_id)
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, message_new_id)
         sr = SatelliteRhc.objects.get(instance_id='357b7360-c0d6-11ec-a1f5-abea1b2200b3')
         self.assertEqual(sr.source_id, 1337)
         self.assertEqual(sr.rhc_client_id, UUID('52321130-c0d6-11ec-a1f5-abea1b2200b3'))
@@ -189,7 +190,7 @@ class TaskSatTrackingTestCase(TestCase):
     def test_non_satellite_source_type(self):
         message_new_source_id = satellite_source_message()
         message_new_source_id['source_type_id'] = -1
-        handle_sources_event(kafka_settings.WEBHOOKS_TOPIC, message_new_source_id)
+        async_to_sync(handle_sources_event)(kafka_settings.WEBHOOKS_TOPIC, message_new_source_id)
 
         sr_count = SatelliteRhc.objects.filter(instance_id='357b7360-c0d6-11ec-a1f5-abea1b2200b3').count()
         self.assertEqual(sr_count, 0)
diff --git a/api/advisor/tasks/tests/test_task_script_upload.py b/api/advisor/tasks/tests/test_task_script_upload.py
index 45ffae8..8f415fa 100644
--- a/api/advisor/tasks/tests/test_task_script_upload.py
+++ b/api/advisor/tasks/tests/test_task_script_upload.py
@@ -15,6 +15,7 @@
 # with Insights Advisor. If not, see <https://www.gnu.org/licenses/>.
 
 import responses
+from asgiref.sync import async_to_sync
 from django.test import TestCase
 
 from project_settings import kafka_settings as kafka_settings
@@ -72,7 +73,7 @@ class TaskJobUpdateScriptUploadTestCase(TestCase):
             json=message
         )
 
-        handle_script_job_updates(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
+        async_to_sync(handle_script_job_updates)(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
 
         self.assertEqual(len(responses.calls), 1)
         job = Job.objects.get(run_id=message['correlation_id'])
@@ -103,7 +104,7 @@ class TaskJobUpdateScriptUploadTestCase(TestCase):
             json=message
         )
 
-        handle_script_job_updates(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
+        async_to_sync(handle_script_job_updates)(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
 
         self.assertEqual(len(responses.calls), 1)
         job = Job.objects.get(run_id=message['correlation_id'])
@@ -123,7 +124,7 @@ class TaskJobUpdateScriptUploadTestCase(TestCase):
             json=message
         )
 
-        handle_script_job_updates(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
+        async_to_sync(handle_script_job_updates)(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
 
         self.assertEqual(len(responses.calls), 1)
         job = Job.objects.get(run_id=message['correlation_id'])
@@ -148,7 +149,7 @@ class TaskJobUpdateScriptUploadTestCase(TestCase):
             json=message
         )
 
-        handle_script_job_updates(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
+        async_to_sync(handle_script_job_updates)(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
 
         self.assertEqual(len(responses.calls), 1)
         job = Job.objects.get(run_id=message['correlation_id'])
@@ -176,7 +177,7 @@ class TaskJobUpdateScriptUploadTestCase(TestCase):
             json=message
         )
 
-        handle_script_job_updates(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
+        async_to_sync(handle_script_job_updates)(kafka_settings.WEBHOOKS_TOPIC, ingress_kafka_message())
 
         self.assertEqual(len(responses.calls), 1)
         job = Job.objects.get(run_id=message['correlation_id'])
